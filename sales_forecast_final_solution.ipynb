{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Forecast using Machine Learning\n",
    "\n",
    "This notebook outlines the process of building a machine learning model to forecast weekly sales for a retail company. The solution achieved **3rd place** in the \"Predykcja sprzedaży z pomocą Machine Learning\" Kaggle competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "The primary goal of this project is to accurately predict `weekly_sales` for various store-department combinations. The solution leverages historical sales data along with supplementary information about stores, economic indicators, and promotional markdowns. The modeling approach involves extensive feature engineering, the use of gradient boosting models (XGBoost and CatBoost), and hyperparameter optimization to achieve high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Setup\n",
    "\n",
    "First, we'll load the necessary libraries and the datasets. The data is split into multiple files: historical sales, features (like temperature and fuel price), and store information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Libraries\n",
    "\n",
    "We begin by importing the essential Python libraries for data manipulation, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "from functools import partial\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import scikitplot as skplt\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data Loading\n",
    "\n",
    "Here we load the training, testing, features, and store datasets. We also perform initial data type conversions for date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from the same directory structure as the original notebook\n",
    "df_sales_train = pd.read_hdf(\"../input/sales_train.h5\")\n",
    "df_sales_test = pd.read_hdf(\"../input/sales_test.h5\")\n",
    "df_features_train = pd.read_hdf(\"../input/features_train.h5\")\n",
    "df_features_test = pd.read_hdf(\"../input/features_test.h5\")\n",
    "df_store = pd.read_csv(\"../input/stores_data.csv\")\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "df_sales_train[\"date\"] = pd.to_datetime(df_sales_train[\"date\"], format='%d/%m/%Y')\n",
    "df_sales_test[\"date\"] = pd.to_datetime(df_sales_test[\"date\"], format='%d/%m/%Y')\n",
    "df_features_train[\"date\"] = pd.to_datetime(df_features_train[\"date\"], format='%d/%m/%Y')\n",
    "df_features_test[\"date\"] = pd.to_datetime(df_features_test[\"date\"], format='%d/%m/%Y')\n",
    "\n",
    "# Prepare store data\n",
    "df_store.columns = [\"store\", \"type\", \"size\"]\n",
    "df_store['type_cat'] = df_store['type'].factorize()[0]\n",
    "df_store.drop(columns=['type'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "This is a critical step where we create new features to improve model performance. A key part of this process is the `simple_feature_engineering` function, which encapsulates all our feature creation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Target Variable Transformation\n",
    "\n",
    "To stabilize variance and handle the positive nature of sales data, we apply a logarithmic transformation to the `weekly_sales` target variable. An offset is used to handle zero or negative sales values if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_target(y, offset):\n",
    "    \"\"\"Applies a log transformation to the target variable.\"\"\"\n",
    "    return np.log(y - offset)\n",
    "\n",
    "def inverse_log_transform_target(log_y, offset):\n",
    "    \"\"\"Reverses the log transformation.\"\"\"\n",
    "    return np.exp(log_y) + offset\n",
    "\n",
    "# The offset is based on the minimum value in the training set to avoid log(0) or log(negative)\n",
    "offset = df_sales_train['weekly_sales'].min() - 1\n",
    "df_sales_train['log_weekly_sales'] = df_sales_train['weekly_sales'].map(lambda x: log_transform_target(x, offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Comprehensive Feature Creation\n",
    "\n",
    "The `simple_feature_engineering` function creates a rich set of features, including:\n",
    "- Time-based features (month, year, week, etc.).\n",
    "- A unique identifier for each store-department combination.\n",
    "- Aggregated sales statistics from the previous year.\n",
    "- Store and department-level sales statistics (mean, std, median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_feature_engineering(df, is_train=True, train_df=None):\n",
    "    \"\"\"\n",
    "    Creates a set of new features from the existing data.\n",
    "    `is_train` controls whether to compute stats (should only be done on training data).\n",
    "    `train_df` is the original training dataframe, used to compute stats for the test set.\n",
    "    \"\"\"\n",
    "    if train_df is None:\n",
    "        train_df = df\n",
    "\n",
    "    # Date features\n",
    "    df['month'] = df[\"date\"].dt.month\n",
    "    df['year'] = df[\"date\"].dt.year\n",
    "    df['week'] = df[\"date\"].dt.isocalendar().week\n",
    "    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "    df[\"dayofyear\"] = df[\"date\"].dt.dayofyear\n",
    "\n",
    "    # Interaction feature\n",
    "    df['id-store-dept'] = df['store'].astype(str) + '_' + df['dept'].astype(str)\n",
    "\n",
    "    # Store-level features from the training set\n",
    "    temp_sum_dept = train_df.groupby('store')['dept'].nunique()\n",
    "    df['sum_dept'] = df['store'].map(temp_sum_dept)\n",
    "\n",
    "    # Merge store information (size, type)\n",
    "    df = pd.merge(df, df_store, on=['store'], how='left')\n",
    "\n",
    "    # --- Lagged and Statistical Features (computed from training data) ---\n",
    "    df[\"year_minus_1\"] = df[\"year\"] - 1\n",
    "\n",
    "    # Previous year's weekly average sales\n",
    "    df_sales_group = train_df.groupby([\"id-store-dept\", \"year\", \"week\"]).agg(PY_week_log_sales=('log_weekly_sales', 'mean')).reset_index()\n",
    "    df_sales_group = df_sales_group.rename(columns={'year': 'year_minus_1'})\n",
    "    df = pd.merge(df, df_sales_group, on=['id-store-dept', 'year_minus_1', 'week'], how='left')\n",
    "\n",
    "    # Previous year's monthly average sales\n",
    "    df_sales_group_2 = train_df.groupby([\"id-store-dept\", \"year\", \"month\"]).agg(PY_month_log_sales=('log_weekly_sales', 'mean')).reset_index()\n",
    "    df_sales_group_2 = df_sales_group_2.rename(columns={'year': 'year_minus_1'})\n",
    "    df = pd.merge(df, df_sales_group_2, on=['id-store-dept', 'year_minus_1', 'month'], how='left')\n",
    "\n",
    "    # Overall store sales statistics\n",
    "    df_store_stats = train_df.groupby(\"store\")['log_weekly_sales'].agg(['mean', 'std', 'median', 'size']).reset_index()\n",
    "    df_store_stats.columns = ['store', 'store_mean', 'store_std', 'store_median', 'store_size']\n",
    "    df = pd.merge(df, df_store_stats, on=['store'], how='left')\n",
    "\n",
    "    # Overall department-in-store sales statistics\n",
    "    df_store_stats_dept = train_df.groupby(['store', 'dept'])['log_weekly_sales'].agg(['mean', 'std', 'median', 'size']).reset_index()\n",
    "    df_store_stats_dept.columns = ['store', 'dept', 'dept_mean', 'dept_std', 'dept_median', 'dept_size']\n",
    "    df = pd.merge(df, df_store_stats_dept, on=['store', 'dept'], how='left')\n",
    "\n",
    "    # Rolling mean feature (calculated on the original 'weekly_sales')\n",
    "    # This is calculated separately as it's a forward-looking calculation within a group\n",
    "    if is_train:\n",
    "        df[\"rolling_mean_3\"] = df.groupby([\"store\", \"dept\"])[\"weekly_sales\"].transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "    else:\n",
    "        # For test set, we can't compute a rolling mean. We will fill it.\n",
    "        df[\"rolling_mean_3\"] = -1\n",
    "\n",
    "    # Fill any remaining missing values\n",
    "    df.fillna(-1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to the training set\n",
    "df_sales_train = simple_feature_engineering(df_sales_train, is_train=True)\n",
    "\n",
    "# Apply feature engineering to the test set, using stats from the training set\n",
    "df_sales_test = simple_feature_engineering(df_sales_test, is_train=False, train_df=df_sales_train)\n",
    "\n",
    "# --- Merge external features (Temperature, CPI, etc.) ---\n",
    "feat_vars_train = ['store', 'date', 'temperature', 'fuel_price', 'cpi', 'unemployment']\n",
    "df_sales_train = pd.merge(df_sales_train, df_features_train[feat_vars_train], on=['store', 'date'], how='left')\n",
    "\n",
    "# For the test set, we merge the external features based on the date\n",
    "# If features for a specific test date aren't available, we'll forward-fill them later\n",
    "df_sales_test = pd.merge(df_sales_test, df_features_test[feat_vars_train], on=['store', 'date'], how='left')\n",
    "\n",
    "# Fill any NaNs created by merges (e.g., if test dates don't have features)\n",
    "df_sales_train.fillna(-1, inplace=True)\n",
    "df_sales_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "With our feature-rich dataset, we now move to the modeling phase. We'll use two powerful gradient boosting models: **XGBoost** and **CatBoost**. We will also use GPU acceleration for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Model and Evaluation Functions\n",
    "\n",
    "We define a set of helper functions to streamline model training, cross-validation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(xgb_params, ctb_params):\n",
    "    \"\"\"Returns a list of models to be trained.\"\"\"\n",
    "    return [\n",
    "        ('xgb', xgb.XGBRegressor(**xgb_params)),\n",
    "        ('ctb', ctb.CatBoostRegressor(**ctb_params))\n",
    "    ]\n",
    "\n",
    "def fit_and_predict(model, X_train, y_train, X_test, offset):\n",
    "    \"\"\"Fits the model on log-transformed data and makes predictions.\"\"\"\n",
    "    y_train_log = log_transform_target(y_train, offset)\n",
    "    model.fit(X_train, y_train_log)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    return inverse_log_transform_target(y_pred_log, offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature Importance Analysis\n",
    "\n",
    "Before full training, we analyze feature importance using `eli5`'s Permutation Importance on a sample of the data. This helps us understand which features are most influential and can guide feature selection. We use a base model for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a temporary sample for faster importance calculation\n",
    "temp_df_train = df_sales_train.sample(n=50000, random_state=42) \n",
    "\n",
    "# Define all potential features, excluding identifiers and target variables\n",
    "all_feats = [col for col in temp_df_train.columns if col not in \n",
    "             ['id', 'weekly_sales', 'log_weekly_sales', 'date', 'year_minus_1', 'id-store-dept']]\n",
    "\n",
    "def plot_feature_importance(X, y, offset, features, model):\n",
    "    \"\"\"Calculates and plots feature importance.\"\"\"\n",
    "    y_log = log_transform_target(y, offset)\n",
    "    \n",
    "    # Use Permutation Importance for a more robust measure\n",
    "    perm = PermutationImportance(model, random_state=0, n_iter=2).fit(X, y_log)\n",
    "    \n",
    "    # Display results\n",
    "    display(eli5.show_weights(perm, feature_names=features, top=30))\n",
    "\n",
    "# Define base models for importance check\n",
    "base_xgb = xgb.XGBRegressor(n_estimators=100, random_state=0, tree_method='gpu_hist')\n",
    "base_ctb = ctb.CatBoostRegressor(n_estimators=100, random_state=0, verbose=False, task_type='GPU')\n",
    "\n",
    "# Get feature values\n",
    "X_temp = temp_df_train[all_feats].values\n",
    "y_temp = temp_df_train['weekly_sales'].values\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "print(\"--- XGBoost Feature Importance ---\")\n",
    "plot_feature_importance(X_temp, y_temp, offset, all_feats, base_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Feature Importance\n",
    "print(\"\\n--- CatBoost Feature Importance ---\")\n",
    "plot_feature_importance(X_temp, y_temp, offset, all_feats, base_ctb)\n",
    "\n",
    "# Based on the feature importance analysis, we select a curated list of the most impactful features.\n",
    "# This reduces model complexity and training time.\n",
    "final_feats = [\n",
    "    'dept_mean',\n",
    "    'rolling_mean_3',\n",
    "    'PY_week_log_sales',\n",
    "    'dept_median',\n",
    "    'PY_month_log_sales',\n",
    "    'dept_std',\n",
    "    'week',\n",
    "    'dept',\n",
    "    'dayofyear',\n",
    "    'dept_size',\n",
    "    'fuel_price',\n",
    "    'year',\n",
    "    'temperature',\n",
    "    'month',\n",
    "    'size',\n",
    "    'cpi',\n",
    "    'store_size',\n",
    "    'store_median'\n",
    "]\n",
    "\n",
    "print(f\"\\nSelected {len(final_feats)} features for the final model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Hyperopt\n",
    "\n",
    "To find the optimal hyperparameters for our models, we use **Hyperopt**, a library for Bayesian optimization. The code for this is shown below but commented out, as it is computationally expensive. We will use the optimized parameters found during the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following parameters are the result of the hyperparameter tuning from the competition\n",
    "xgb_best_params = {\n",
    "    'colsample_bytree': 0.8057199686605526,\n",
    "    'learning_rate': 0.16139979310707386,\n",
    "    'max_depth': 14,\n",
    "    'random_state': 4150,\n",
    "    'subsample': 0.939396580073127,\n",
    "    'n_estimators': 100, # Kept low for speed, can be increased with early stopping\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0\n",
    "}\n",
    "\n",
    "ctb_best_params = {\n",
    "    'depth': 14,\n",
    "    'random_state': 10000,\n",
    "    'n_estimators': 100, # Kept low for speed\n",
    "    'verbose': False,\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0'\n",
    "}\n",
    "\n",
    "print(\"Optimized hyperparameters have been loaded.\")\n",
    "\n",
    "# --- Example Hyperopt implementation (run during competition, not executed here) ---\n",
    "# space_xgb = {\n",
    "#     'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
    "#     'colsample_bytree': hp.uniform('colsample_bytree', 0.8, 1.0),\n",
    "#     # ... other params\n",
    "# }\n",
    "\n",
    "# def objective_xgb(space):\n",
    "#     X, y = df_sales_train[final_feats].values, df_sales_train['weekly_sales'].values\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#     model_params = {\n",
    "#         'max_depth': int(space['max_depth']),\n",
    "#         'colsample_bytree': space['colsample_bytree'],\n",
    "#         # ... other params\n",
    "#         'n_estimators': 100, 'tree_method': 'gpu_hist'\n",
    "#     }\n",
    "#     y_pred = fit_and_predict(xgb.XGBRegressor(**model_params), X_train, y_train, X_test, offset)\n",
    "#     score = mean_absolute_error(y_test, y_pred)\n",
    "#     return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "# print(\"Best XGBoost params: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Training and Prediction\n",
    "\n",
    "With the best hyperparameters identified, we train our final models on the entire training dataset and generate predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final datasets with the selected features\n",
    "X_train = df_sales_train[final_feats].values\n",
    "X_test = df_sales_test[final_feats].values\n",
    "y_train = df_sales_train['weekly_sales'].values\n",
    "\n",
    "# Train and predict with XGBoost\n",
    "print(\"Training final XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(**xgb_best_params)\n",
    "y_pred_xgb = fit_and_predict(xgb_model, X_train, y_train, X_test, offset)\n",
    "print(\"XGBoost prediction complete.\")\n",
    "\n",
    "# Train and predict with CatBoost\n",
    "print(\"\\nTraining final CatBoost model...\")\n",
    "ctb_model = ctb.CatBoostRegressor(**ctb_best_params)\n",
    "y_pred_ctb = fit_and_predict(ctb_model, X_train, y_train, X_test, offset)\n",
    "print(\"CatBoost prediction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Model Ensembling\n",
    "\n",
    "To further improve our predictions and reduce variance, we create an ensemble by averaging the predictions from the XGBoost and CatBoost models. This simple approach is often very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the predictions from both models\n",
    "y_pred_ensemble = (y_pred_xgb + y_pred_ctb) / 2\n",
    "print(\"Ensemble predictions created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission\n",
    "\n",
    "Finally, we format our predictions into the required submission file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "df_sales_test['weekly_sales'] = y_pred_ensemble\n",
    "\n",
    "# Ensure output directory exists\n",
    "!mkdir -p ../output\n",
    "\n",
    "# Save to CSV\n",
    "submission_path = \"../output/mean_xgb_cbt_hyperopt_v1.csv\"\n",
    "df_sales_test[[\"id\", \"weekly_sales\"]].to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved to: {submission_path}\")\n",
    "display(df_sales_test[[\"id\", \"weekly_sales\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This project successfully demonstrates a robust pipeline for sales forecasting. Key takeaways include:\n",
    "- **The power of feature engineering**: Creating insightful features like rolling statistics and lagged sales from the previous year was crucial for model performance.\n",
    "- **The effectiveness of gradient boosting**: XGBoost and CatBoost proved to be highly effective for this tabular data problem, and using GPUs significantly sped up training.\n",
    "- **The benefit of ensembling**: Combining the predictions of multiple diverse models led to a more robust and accurate final prediction.\n",
    "- **Structured methodology**: A clear, step-by-step process of data prep, feature engineering, importance analysis, tuning, and final modeling was instrumental in achieving a top-3 position in the competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
